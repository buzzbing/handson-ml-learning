{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mnist['data']))\n",
    "display(mnist['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, taking mnist['data'] i.e. pixel values of images as X and the target data i.e. the numbers the images represent as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = mnist['data'], mnist['target']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70,000 images, and each image has 784 features. This is because each image\n",
    "is 28×28 pixels, and each feature simply represents one pixel’s intensity, from 0\n",
    "(white) to 255 (black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "some_digit = X[0:1]\n",
    "some_digit_image = some_digit.values.reshape(28,28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation=\"nearest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 5-detector that classifies whether the given number is 5 or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_5 = (y_train == 5)\n",
    "y_test_5 = (y_test == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Classifier(SGDClassifier) deals with training instances independently one at a time. It relies on randomness during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_clf = SGDClassifier(random_state= 42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.predict(some_digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures\n",
    "\n",
    "#### 1. Measuring ***Accuracy*** Using **Cross-Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import clone\n",
    "\n",
    "skfolds = StratifiedKFold(n_splits=3, random_state=42, shuffle=True)\n",
    "\n",
    "for k,v in skfolds.split(X_train, y_train_5):\n",
    "    print('train_idx:',k,'test_idx:',v)\n",
    "print(type(k), type(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for train_index, test_index in skfolds.split(X_train, y_train_5):\n",
    "    # clone functions creates a clone of already existing trained model\n",
    "    clone_clf = clone(sgd_clf)\n",
    "    X_train_folds = X_train.iloc[train_index]\n",
    "    y_train_folds = y_train_5.iloc[train_index]\n",
    "    X_test_fold = X_train.iloc[test_index]\n",
    "    y_test_fold = y_train_5.iloc[test_index]\n",
    "\n",
    "    clone_clf.fit(X_train_folds, y_train_folds)\n",
    "    y_pred = clone_clf.predict(X_test_fold)\n",
    "    n_correct = sum(y_pred == y_test_fold)\n",
    "    print(n_correct/len(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above process can be carried out directly by using **cross_val_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy not a good performance matrix for skewed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ***Confusion Matrix***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train,y_train_5,cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the cross_val_score() function, cross_val_predict() performs K-fold\n",
    "cross-validation, but instead of returning the evaluation scores, it returns the predictions made on each test fold. This means that you get a clean prediction for each\n",
    "instance in the training set (“clean” meaning that the prediction is made by a model\n",
    "that never saw the data during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_train_5, y_train_pred)\n",
    "# [[TN, FP],\n",
    "# [FN, TP]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ***Precision*** and ***Recall***\n",
    "\n",
    "precision = TP/(TP+FP) i.e. true positive / predicted positives\n",
    "recall = TPR = sensitivity = TP/(TP + FN) i.e. true positive / actual positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "print(precision_score(y_train_5, y_train_pred))\n",
    "print(recall_score(y_train_5, y_train_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you trained a classifier to detect videos that are safe for kids, you would probably prefer a classifier that rejects many good videos (low recall) but keeps only safe ones (high precision)\n",
    "\n",
    "Suppose you train a classifier to detect\n",
    "shoplifters on surveillance images: it is probably fine if your classifier has only 30%\n",
    "precision as long as it has 99% recall (sure, the security guards will get a few false\n",
    "alerts, but almost all shoplifters will get caught)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ***F1 Score***\n",
    "\n",
    "harmonic mean of precision and recall\n",
    "\n",
    "F1 score gets high value only when both precision and recall have high values\n",
    "\n",
    "F1 Score = 2 * (precision * recall)/(precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(y_train_5, y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Precision and Recall Tradeoffs***\n",
    "\n",
    "As the precision increases, the recall score decreases and vice versa\n",
    "\n",
    "To see this, instead of calling the classifier’s\n",
    "predict() method, you can call its decision_function() method, which returns a\n",
    "score for each instance, and then make predictions based on those scores using any\n",
    "threshold you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scores = sgd_clf.decision_function(some_digit)\n",
    "print(y_scores)\n",
    "\n",
    "threshold = 0\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "print(y_some_digit_pred)\n",
    "\n",
    "\n",
    "threshold=8000\n",
    "y_some_digit_pred = (y_scores > threshold)\n",
    "print(y_some_digit_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms that raising the threshold decreases recall. The image actually repre‐\n",
    "sents a 5, and the classifier detects it when the threshold is 0, but it misses it when the\n",
    "threshold is increased to 8,000.\n",
    "\n",
    "To obtain the right threshold **precision_recall_curve** is used\n",
    "\n",
    "For this you will first need to get the\n",
    "scores of all instances in the training set using the cross_val_predict() function\n",
    "again, but this time specifying that you want it to return decision scores instead of\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method='decision_function')\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n",
    "    plt.plot(thresholds, recalls[:-1], 'g-',label='Recall')\n",
    "\n",
    "plot_precision_recall_threshold(precisions, recalls, thresholds)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision curve is bumpier than the recall\n",
    "curve because precision may sometimes go\n",
    "down when you raise the threshold (although in general it will go\n",
    "up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_with_90_precision = thresholds[np.argmax(precisions >=0.9)]\n",
    "print('threshold with 90% precision:    ',threshold_with_90_precision)\n",
    "\n",
    "y_train_pred_90 = (y_scores >= threshold_with_90_precision)\n",
    "print('precision score:    ',precision_score(y_train_5, y_train_pred_90))\n",
    "print('recall_score:    ',recall_score(y_train_5, y_train_pred_90))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ***ROC Curve***\n",
    "\n",
    "**Receiver Operating Characteristics** curve is used to plot True Positive Rate(TPR) against False Positive Rate(FPR)\n",
    "\n",
    "FPR = 1- TNR(SPECIFICITY)\n",
    "\n",
    "Hence, ROC curve = plot of TPR VS 1-specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "fpr,tpr, thresholds = roc_curve(y_train_5, y_scores)\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr,tpr, label=label)\n",
    "    plt.plot([0,1],[0,1],'g--')\n",
    "\n",
    "plot_roc_curve(fpr,tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the TPR, the higher FPR. The dotted line represents the ROC curve of a purely\n",
    "random classifier; a good classifier stays as far away from that line as possible (toward\n",
    "the top-left corner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. ***ROC AUC SCORE***\n",
    "\n",
    "AUC = Area Under Curve\n",
    "\n",
    "ROC AUC = Area Under Receiver Operating Characteristic Curve\n",
    "\n",
    "Perfect classifier has ROC AUC = 1\n",
    "\n",
    "Purely random classifier has ROC AUC = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_train_5,y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As a rule\n",
    "of thumb, you should prefer the Precision/Recall curve whenever the positive\n",
    "class is rare or when you care more about the false positives than\n",
    "the false negatives, and the ROC curve otherwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing SGD and Random Forest Classifiers ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(forest_clf, X_train,y_train_5,cv=3, method = 'predict_proba')\n",
    "y_scores_forest = y_probas_forest[:,1]\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)\n",
    "plt.plot(fpr,tpr,\"b:\",label=\"SGD\")\n",
    "plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\") \n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
